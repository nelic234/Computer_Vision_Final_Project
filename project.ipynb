{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec08ffb",
   "metadata": {},
   "source": [
    "# Final Project Computer Vision\n",
    "## Truth in Motion: Depth and Flow Enhanced DeepFake Detection\n",
    "\n",
    "Authors: Aimee Lin, Neli Catar and Gellert Toth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316f18a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a0a224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Depth-Anything-V2'...\n",
      "remote: Enumerating objects: 142, done.\u001b[K\n",
      "remote: Total 142 (delta 0), reused 0 (delta 0), pack-reused 142 (from 1)\u001b[K\n",
      "Receiving objects: 100% (142/142), 45.17 MiB | 6.56 MiB/s, done.\n",
      "Resolving deltas: 100% (46/46), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:DepthAnything/Depth-Anything-V2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20779f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1140  100  1140    0     0   7116      0 --:--:-- --:--:-- --:--:--  7125\n",
      "100  371M  100  371M    0     0  15.3M      0  0:00:24  0:00:24 --:--:-- 19.9M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p DepthAnythingV2Checkpoints\n",
    "!curl -L https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth?download=true --output DepthAnythingV2Checkpoints/depth_anything_v2_vitb.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c0e141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-pwc'...\n",
      "remote: Enumerating objects: 251, done.\u001b[K\n",
      "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
      "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
      "remote: Total 251 (delta 40), reused 62 (delta 24), pack-reused 169 (from 1)\u001b[K\n",
      "Receiving objects: 100% (251/251), 66.96 MiB | 6.95 MiB/s, done.\n",
      "Resolving deltas: 100% (120/120), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:sniklaus/pytorch-pwc.git\n",
    "!mv pytorch-pwc pytorch_pwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torchvision.transforms as T\n",
    "import importlib.util\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from IPython.display import display\n",
    "from matplotlib import colors\n",
    "from PIL import Image, ImageShow\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "depth_anything_path = os.path.abspath(\"Depth-Anything-V2\")\n",
    "\n",
    "if depth_anything_path not in sys.path:\n",
    "    sys.path.append(depth_anything_path)\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "argv_backup = sys.argv\n",
    "sys.argv = [sys.argv[0]] \n",
    "from pytorch_pwc.run import estimate \n",
    "sys.argv = argv_backup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db574a14",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4349aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# data folders\n",
    "video_root = './data' # where the data is stored\n",
    "data_root = './preprocessed_data' # where the pre processed data gets stored\n",
    "data_flow = \"./flow_data\" # where flow data gets saved\n",
    "data_depth = \"./depth_data\"\n",
    "\n",
    "# data loading\n",
    "BATCH_SIZE = 64 # batch size of the preprocessed data\n",
    "\n",
    "# device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "# training\n",
    "num_epochs = 5 # epochs when training\n",
    "SKIP_FRAMES = 2 # When processing the video jump some frames to reduce data size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5106e7",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_into_frames(path, save_path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    faces = [frame_faces for i in range(0, len(frames), BATCH_SIZE) for frame_faces in mtcnn(frames[i:min(len(frames),i+BATCH_SIZE)])]\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, frame_faces in enumerate(faces):\n",
    "        if frame_faces is None:\n",
    "            continue\n",
    "        face = frame_faces[0]\n",
    "        img_pil = Image.fromarray(face.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "        img_pil.save(os.path.join(save_path, f\"frame_{i}.png\"))\n",
    "\n",
    "def list_leaf_folders(root_dir):\n",
    "    leaf_folders = []\n",
    "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
    "        if not dirnames:\n",
    "            leaf_folders.append(dirpath)\n",
    "    return leaf_folders\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def list_sorted_images(folder, extensions={'.png', '.jpg', '.jpeg', '.bmp'}):\n",
    "    files = [f for f in os.listdir(folder)\n",
    "             if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in extensions]\n",
    "    files.sort(key=natural_sort_key)\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "def flow_to_rgb(flow):\n",
    "    u = flow[..., 0]\n",
    "    v = flow[..., 1] \n",
    "\n",
    "    magnitude = np.sqrt(u**2 + v**2) \n",
    "    direction = np.arctan2(v, u)\n",
    "    mag_max = np.max(magnitude)\n",
    "    magnitude = magnitude / mag_max if mag_max != 0 else magnitude\n",
    "\n",
    "    direction = (direction + np.pi) / (2 * np.pi)\n",
    "    saturation = magnitude\n",
    "    value = np.ones_like(magnitude)\n",
    "    hsv_image = np.stack((direction, saturation, value), axis=-1)\n",
    "    rgb_image = colors.hsv_to_rgb(hsv_image.astype(np.float32))\n",
    "    return rgb_image\n",
    "\n",
    "def calculate_flow_picture(img1, img2):\n",
    "    res = estimate(img1, img2)\n",
    "    res = np.array(res.numpy(force=True).transpose(1, 2, 0), np.float32)\n",
    "    res = flow_to_rgb(res)\n",
    "    return res\n",
    "\n",
    "def calculate_depth_picture(model, img):\n",
    "    res = model.infer_image(img)\n",
    "    depth_min = np.min(res)\n",
    "    depth_max = np.max(res)\n",
    "    if depth_max - depth_min < 1e-6:\n",
    "        normalised = np.zeros_like(res, dtype=np.uint8)\n",
    "    else:\n",
    "        normalised = 255 * (res - depth_min) / (depth_max - depth_min)\n",
    "        normalised = normalised.astype(np.uint8)\n",
    "    return normalised\n",
    "\n",
    "def collect_video_folders(base_dir):\n",
    "    folders = []\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            # check if it contains frames\n",
    "            if any(fname.lower().endswith(('.jpg', '.png')) for fname in os.listdir(folder_path)):\n",
    "                folders.append(folder_path)\n",
    "    return sorted(folders)\n",
    "\n",
    "def extract_frame_paths_and_labels(base_dirs, label, train_ratio=0.8):\n",
    "    train_paths = []\n",
    "    test_paths = []\n",
    "\n",
    "    for base_dir in base_dirs:\n",
    "        video_folders = collect_video_folders(base_dir)\n",
    "        total_videos = len(video_folders)\n",
    "\n",
    "        train_cutoff = int(total_videos * train_ratio)\n",
    "\n",
    "        for i, video_folder in enumerate(video_folders):\n",
    "            frame_paths = glob.glob(os.path.join(video_folder, '*'))\n",
    "            frame_paths = [f for f in frame_paths if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "            labeled = [(fp, label) for fp in frame_paths]\n",
    "\n",
    "            if i < train_cutoff:\n",
    "                train_paths.extend(labeled)\n",
    "            else:\n",
    "                test_paths.extend(labeled)\n",
    "\n",
    "    return train_paths, test_paths\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label, dtype=torch.float32) \n",
    "        return image, label\n",
    "    \n",
    "def repeat_channels(x):\n",
    "    if isinstance(x, torch.Tensor) and x.dim() == 3 and x.shape[0] == 1:\n",
    "        return x.repeat(3, 1, 1)\n",
    "    elif isinstance(x, torch.Tensor) and x.dim() == 2:\n",
    "        # Add channel dimension if missing\n",
    "        return x.unsqueeze(0).repeat(3, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected input shape for repeat_channels: {x.shape}\")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, scale=True):\n",
    "    if scale:\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        if scale:\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = probs > 0.5\n",
    "        correct += (preds == labels.bool()).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        all_labels.append(labels.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).detach().numpy()\n",
    "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs > 0.5\n",
    "            correct += (preds == labels.bool()).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
    "\n",
    "def plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs):\n",
    "    epochs = range(1, len(train_accuracies)+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Acc')\n",
    "    plt.plot(epochs, val_accuracies, label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    plt.xticks(list(epochs))\n",
    "    plt.ylim(0.5, 1.0)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_aurocs, label='Train AUROC')\n",
    "    plt.plot(epochs, val_aurocs, label='Val AUROC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUROC')\n",
    "    plt.legend()\n",
    "    plt.title('AUROC')\n",
    "    plt.xticks(list(epochs))\n",
    "    plt.ylim(0.5, 1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_fp16_distilattion(model_learner, model_teacher, dataloader, criterion, optimizer, device):\n",
    "    scaler = GradScaler(device)\n",
    "    model_learner.train()\n",
    "    model_teacher.train()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(dataloader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                cls_token_teacher = model_teacher(images)\n",
    "                cls_token_teacher = cls_token_teacher.detach()\n",
    "        with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            cls_token_teacher_projected = model_learner.cls_projector(cls_token_teacher)\n",
    "            cls_token, outputs  = model_learner(images)\n",
    "\n",
    "            loss = criterion(outputs, labels, cls_token, cls_token_teacher_projected)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = probs > 0.5\n",
    "        correct += (preds == labels.bool()).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        all_labels.append(labels.cpu())\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
    "\n",
    "def evaluate_fp16_distilattion(model_learner, model_teacher, dataloader, criterion, device):\n",
    "    model_teacher.eval()\n",
    "    model_learner.eval()\n",
    "    all_labels, all_probs = [], []\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(dataloader):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                cls_token_teacher = model_teacher(images)\n",
    "                cls_token_teacher = cls_token_teacher.detach()\n",
    "                cls_token_teacher_projected = model_learner.cls_projector(cls_token_teacher)\n",
    "                cls_token, outputs = model_learner(images)\n",
    "\n",
    "                loss = criterion(outputs, labels, cls_token, cls_token_teacher_projected)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs > 0.5\n",
    "            correct += (preds == labels.bool()).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_labels.append(labels.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.distill_loss = self._kl_loss\n",
    "\n",
    "    def forward(self, student_output, labels, student_cls, teacher_cls):\n",
    "        cls_loss = self.bce(student_output, labels)\n",
    "        distill = self.distill_loss(student_cls, teacher_cls)\n",
    "        total_loss = self.alpha * cls_loss + (1 - self.alpha) * distill\n",
    "        return total_loss\n",
    "\n",
    "    def _kl_loss(self, student_cls, teacher_cls):\n",
    "        T = self.temperature\n",
    "        student_log_probs = F.log_softmax(student_cls / T, dim=-1)\n",
    "        teacher_probs = F.softmax(teacher_cls / T, dim=-1)\n",
    "        return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b857a2",
   "metadata": {},
   "source": [
    "#### Feature extraction generation\n",
    "\n",
    "In the sections below we tested the feature extraction generation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7c0c7",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Code for downloading the data is based on FaceForensics++ dataset and is available on https://github.com/ondyari/FaceForensics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3304f",
   "metadata": {},
   "source": [
    "1. Splitting the videos into frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device=DEVICE)\n",
    "\n",
    "for dirpath, _, filenames in os.walk(video_root):\n",
    "    if len(filenames) == 0:\n",
    "        continue\n",
    "    print(f\"Processing videos in {dirpath}\")\n",
    "    for file in tqdm(filenames):\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(dirpath, file)\n",
    "            relative_path = os.path.relpath(video_path, video_root)\n",
    "            output_folder = os.path.join(data_root, os.path.splitext(relative_path)[0])\n",
    "            split_video_into_frames(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f238c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. Calculating Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_folders = list_leaf_folders(data_root)\n",
    "print(leaf_folders)\n",
    "for folder in tqdm(leaf_folders):\n",
    "    relative_path = os.path.relpath(folder, data_root)\n",
    "    images = list_sorted_images(folder)\n",
    "    images = [torch.FloatTensor(np.ascontiguousarray(np.array(Image.open(image))[:, :, ::-1].transpose(2, 0, 1).astype(np.float32) * (1.0 / 255.0))) for image in images]\n",
    "    for i in range(0, len(images)-1, SKIP_FRAMES):\n",
    "        img1, img2 = images[i], images[i+1]\n",
    "        res = calculate_flow_picture(img1, img2)\n",
    "        save_path = os.path.join(data_flow, relative_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        rgb_flow_uint8 = (res * 255).astype(np.uint8)\n",
    "        pil_img = Image.fromarray(rgb_flow_uint8, mode='RGB')\n",
    "        pil_img.save(os.path.join(save_path, f\"flow_image_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d204d08",
   "metadata": {},
   "source": [
    "3. Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 270/400 [2:06:04<1:27:07, 40.21s/it]"
     ]
    }
   ],
   "source": [
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "model = DepthAnythingV2(**model_configs['vitb'])\n",
    "model.load_state_dict(torch.load(f'DepthAnythingV2Checkpoints/depth_anything_v2_vitb.pth', map_location='cpu'))\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "leaf_folders = list_leaf_folders(data_root)\n",
    "for folder in tqdm(leaf_folders):\n",
    "    relative_path = os.path.relpath(folder, data_root)\n",
    "    images = list_sorted_images(folder)\n",
    "    for i in range(0, len(images), SKIP_FRAMES):\n",
    "        img_path = images[i]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        res = calculate_depth_picture(model, img)\n",
    "        save_path = os.path.join(data_depth, relative_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        pil_img = Image.fromarray(res, mode='L')  \n",
    "        pil_img.save(os.path.join(save_path, f\"depth_image_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e025ca9",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoBinaryClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.config.hidden_size, 256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1, bias=True)  # binary output\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]  \n",
    "        return self.classifier(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd398ae",
   "metadata": {},
   "source": [
    "#### Flow based network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d16bf",
   "metadata": {},
   "source": [
    "Preparing the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths\n",
    "original_sources = [\n",
    "    'flow_data/original_sequences/actors/c23/videos',\n",
    "    'flow_data/original_sequences/youtube/c23/videos'\n",
    "]\n",
    "\n",
    "manipulated_sources = [\n",
    "    'flow_data/manipulated_sequences/DeepFakeDetection/c23/videos',\n",
    "    'flow_data/manipulated_sequences/Deepfakes/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/Face2Face/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/FaceShifter/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/FaceSwap/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/NeuralTextures/c23/videos'\n",
    "]\n",
    "\n",
    "# Collect data\n",
    "train_real, test_real = extract_frame_paths_and_labels(original_sources, label=0)\n",
    "train_fake, test_fake = extract_frame_paths_and_labels(manipulated_sources, label=1)\n",
    "\n",
    "# Combine\n",
    "train_data = train_real + train_fake\n",
    "test_data = test_real + test_fake\n",
    "\n",
    "# Sample output\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")\n",
    "print(f\"Sample training item: {train_data[0]}\")\n",
    "\n",
    "train_dataset = FrameDataset(train_data, transform=image_transform)\n",
    "test_dataset = FrameDataset(test_data, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/dinov2-base\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "backbone = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in backbone.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"backbone\" in name:\n",
    "            backbone_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "print(len(backbone_params), len(classifier_params))\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"lr\": 1e-4},\n",
    "])\n",
    "\n",
    "model = DinoBinaryClassifier(backbone)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, val_accuracies = [], []\n",
    "train_aurocs, val_aurocs = [], []\n",
    "epoch_times = []\n",
    "\n",
    "start_epoch = 1\n",
    "num_epochs = 5\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    # save_path = f\"depth_swin_epoch_{epoch}.pth\"\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_labels, train_probs = train(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, val_labels, val_probs = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    \n",
    "    # AUROC\n",
    "    train_auroc = roc_auc_score(train_labels, train_probs)\n",
    "    val_auroc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_aurocs.append(train_auroc)\n",
    "    val_aurocs.append(val_auroc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
    "    print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'train_loss': train_loss,\n",
    "    #     'val_loss': val_loss,\n",
    "    # }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs)\n",
    "save_path = '/checkpoints/flow_based_classifier_dinov2_base.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43949af",
   "metadata": {},
   "source": [
    "#### Depth Based network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c41592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your paths\n",
    "original_sources = [\n",
    "    'depth_data/original_sequences/actors/c23/videos',\n",
    "    'depth_data/original_sequences/youtube/c23/videos'\n",
    "]\n",
    "\n",
    "manipulated_sources = [\n",
    "    'depth_data/manipulated_sequences/DeepFakeDetection/c23/videos',\n",
    "    'depth_data/manipulated_sequences/Deepfakes/c23/videos',\n",
    "    # 'depth_data/manipulated_sequences/Face2Face/c23/videos',\n",
    "    # 'depth_data/manipulated_sequences/FaceShifter/c23/videos',\n",
    "    # 'depth_data/manipulated_sequences/FaceSwap/c23/videos',\n",
    "    # 'depth_data/manipulated_sequences/NeuralTextures/c23/videos'\n",
    "]\n",
    "\n",
    "# Collect data\n",
    "train_real, test_real = extract_frame_paths_and_labels(original_sources, label=0)\n",
    "train_fake, test_fake = extract_frame_paths_and_labels(manipulated_sources, label=1)\n",
    "\n",
    "# Combine\n",
    "train_data = train_real + train_fake\n",
    "test_data = test_real + test_fake\n",
    "\n",
    "# Sample output\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")\n",
    "print(f\"Sample training item: {train_data[0]}\")\n",
    "\n",
    "train_dataset = FrameDataset(train_data, transform=image_transform)\n",
    "test_dataset = FrameDataset(test_data, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aadae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/dinov2-base\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "backbone = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in backbone.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"backbone\" in name:\n",
    "            backbone_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "print(len(backbone_params), len(classifier_params))\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"lr\": 1e-4},\n",
    "])\n",
    "\n",
    "model = DinoBinaryClassifier(backbone)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69827b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, val_accuracies = [], []\n",
    "train_aurocs, val_aurocs = [], []\n",
    "epoch_times = []\n",
    "\n",
    "start_epoch = 1\n",
    "num_epochs = 5\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    # save_path = f\"depth_swin_epoch_{epoch}.pth\"\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_labels, train_probs = train(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, val_labels, val_probs = evaluate(model, test_loader, criterion, DEVICE)\n",
    "    \n",
    "    # AUROC\n",
    "    train_auroc = roc_auc_score(train_labels, train_probs)\n",
    "    val_auroc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_aurocs.append(train_auroc)\n",
    "    val_aurocs.append(val_auroc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
    "    print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'train_loss': train_loss,\n",
    "    #     'val_loss': val_loss,\n",
    "    # }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bbac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs)\n",
    "save_path = '/checkpoints/depth_based_classifier_dinov2_base.pth'\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e1aa6",
   "metadata": {},
   "source": [
    "### Compression methods\n",
    "\n",
    "1. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ab1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf08f6aa",
   "metadata": {},
   "source": [
    "2. Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoBinaryClassifierForDistilattion(DinoBinaryClassifier):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__(backbone)\n",
    "        self.cls_projector = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "        return cls_token, self.classifier(cls_token)\n",
    "\n",
    "\n",
    "class DinoBinaryClassifierForDistilattionTeacher(DinoBinaryClassifier):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__(backbone)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "        return cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model_name = \"facebook/dinov2-large\"\n",
    "small_model_name = \"facebook/dinov2-small\"\n",
    "\n",
    "backbone_large = AutoModel.from_pretrained(large_model_name)\n",
    "backbone_small = AutoModel.from_pretrained(small_model_name)\n",
    "\n",
    "for param in backbone_small.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, param in backbone_small.named_parameters():\n",
    "    if \"encoder.layer.11\" in name or \"encoder.layer.10\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "model_teacher = DinoBinaryClassifierForDistilattionTeacher(backbone_large)\n",
    "model_teacher.load_state_dict(torch.load('/checkpoints/depth_based_classifier_dinov2_base.pth'))\n",
    "model_teacher.to(DEVICE)\n",
    "\n",
    "model_learner = DinoBinaryClassifierForDistilattion(backbone_small)\n",
    "model_learner.to(DEVICE)\n",
    "\n",
    "criterion = DistillationLoss()\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model_learner.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"backbone\" in name:\n",
    "            backbone_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "print(len(backbone_params), len(classifier_params))\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"lr\": 1e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a734f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies, val_accuracies = [], []\n",
    "train_aurocs, val_aurocs = [], []\n",
    "epoch_times = []\n",
    "\n",
    "start_epoch = 1\n",
    "num_epochs = 5\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    # save_path = f\"depth_swin_epoch_{epoch}.pth\"\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_labels, train_probs = train_fp16_distilattion(model_learner, model_teacher, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, val_acc, val_labels, val_probs = evaluate_fp16_distilattion(model_learner, model_teacher, test_loader, criterion, DEVICE)\n",
    "    \n",
    "    # AUROC\n",
    "    train_auroc = roc_auc_score(train_labels, train_probs)\n",
    "    val_auroc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_aurocs.append(train_auroc)\n",
    "    val_aurocs.append(val_auroc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
    "    print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
    "    \n",
    "    # torch.save({\n",
    "    #     'epoch': epoch,\n",
    "    #     'model_state_dict': model.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'train_loss': train_loss,\n",
    "    #     'val_loss': val_loss,\n",
    "    # }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d189c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs)\n",
    "save_path = '/checkpoints/flow_based_classifier_dinov2_small_distilled.pth'\n",
    "torch.save(model_learner.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
