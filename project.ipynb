{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec08ffb",
   "metadata": {},
   "source": [
    "# Final Project Computer Vision\n",
    "## Truth in Motion: Depth and Flow Enhanced DeepFake Detection\n",
    "\n",
    "Authors: Aimee Lin, Neli Catar and Gellert Toth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316f18a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from facenet_pytorch import MTCNN\n",
    "from IPython.display import display\n",
    "from matplotlib import colors\n",
    "from PIL import Image, ImageShow\n",
    "from pwcnet.run import estimate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db574a14",
   "metadata": {},
   "source": [
    "### Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4349aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folders\n",
    "video_root = './data' # where the data is stored\n",
    "data_root = './preprocessed_data' # where the pre processed data gets stored\n",
    "data_flow = \"./flow_data\" # where flow data gets saved\n",
    "\n",
    "# data loading\n",
    "BATCH_SIZE_DATA = 64 # batch size of the preprocessed data\n",
    "\n",
    "# device\n",
    "# DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "DEVICE = 'cpu' # setting the default device\n",
    "\n",
    "# training\n",
    "num_epochs = 5 # epochs when training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5106e7",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b857a2",
   "metadata": {},
   "source": [
    "#### Feature extraction generation\n",
    "\n",
    "In the sections below we tested the feature extraction generation techniques\n",
    "\n",
    "##### Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_leaf_folders(root_dir):\n",
    "    leaf_folders = []\n",
    "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
    "        if not dirnames:\n",
    "            leaf_folders.append(dirpath)\n",
    "    return leaf_folders\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def list_sorted_images(folder, extensions={'.png', '.jpg', '.jpeg', '.bmp'}):\n",
    "    files = [f for f in os.listdir(folder)\n",
    "             if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in extensions]\n",
    "    files.sort(key=natural_sort_key)\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "def flow_to_rgb(flow):\n",
    "    \"\"\"\n",
    "    Convert the 2-channel flow output (horizontal and vertical) into an RGB image\n",
    "    where the pixel color represents the angle and saturation represents the magnitude.\n",
    "    \n",
    "    Parameters:\n",
    "        flow (numpy.ndarray): A numpy array of shape (H, W, 2), where:\n",
    "            - flow[..., 0] is the horizontal flow (u)\n",
    "            - flow[..., 1] is the vertical flow (v)\n",
    "    \n",
    "    Returns:\n",
    "        rgb_image (numpy.ndarray): An RGB image (H, W, 3) representing the optical flow\n",
    "    \"\"\"\n",
    "\n",
    "    u = flow[..., 0]\n",
    "    v = flow[..., 1] \n",
    "\n",
    "    magnitude = np.sqrt(u**2 + v**2) \n",
    "    direction = np.arctan2(v, u)\n",
    "    mag_max = np.max(magnitude)\n",
    "    magnitude = magnitude / mag_max if mag_max != 0 else magnitude\n",
    "\n",
    "    direction = (direction + np.pi) / (2 * np.pi)\n",
    "    saturation = magnitude\n",
    "    value = np.ones_like(magnitude)\n",
    "    hsv_image = np.stack((direction, saturation, value), axis=-1)\n",
    "    rgb_image = colors.hsv_to_rgb(hsv_image.astype(np.float32))\n",
    "    return rgb_image\n",
    "\n",
    "def calculate_flow_picture(img1, img2):\n",
    "    res = estimate(img1, img2)\n",
    "    res = np.array(res.numpy(force=True).transpose(1, 2, 0), np.float32)\n",
    "    res = flow_to_rgb(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "leaf_folders = list_leaf_folders(data_root)\n",
    "for folder in tqdm(leaf_folders):\n",
    "    relative_path = os.path.relpath(folder, data_root)\n",
    "    images = list_sorted_images(folder)\n",
    "    images = [torch.FloatTensor(np.ascontiguousarray(np.array(Image.open(image))[:, :, ::-1].transpose(2, 0, 1).astype(np.float32) * (1.0 / 255.0))) for image in images]\n",
    "    for i in range(len(images)-1):\n",
    "        img1, img2 = images[i], images[i+1]\n",
    "        res = calculate_flow_picture(img1, img2)\n",
    "        save_path = os.path.join(data_flow, relative_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        rgb_flow_uint8 = (res * 255).astype(np.uint8)\n",
    "        pil_img = Image.fromarray(rgb_flow_uint8, mode='RGB')\n",
    "        pil_img.save(os.path.join(save_path, f\"flow_image_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d204d08",
   "metadata": {},
   "source": [
    "##### Depth Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "model = DepthAnythingV2(**model_configs['vitb'])\n",
    "model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_vitb.pth', map_location='cpu'))\n",
    "model = model.to(DEVICE).eval()\n",
    "\n",
    "root_dir = \"./preprocessed_data\"\n",
    "save_folder = \"./depth_data\"\n",
    "def list_leaf_folders(root_dir):\n",
    "    leaf_folders = []\n",
    "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
    "        if not dirnames:\n",
    "            leaf_folders.append(dirpath)\n",
    "    return leaf_folders\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "def list_sorted_images(folder, extensions={'.png', '.jpg', '.jpeg', '.bmp'}):\n",
    "    files = [f for f in os.listdir(folder)\n",
    "             if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in extensions]\n",
    "    files.sort(key=natural_sort_key)\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "def calculate_depth_picture(img):\n",
    "    res = model.infer_image(img)\n",
    "    depth_min = np.min(res)\n",
    "    depth_max = np.max(res)\n",
    "    if depth_max - depth_min < 1e-6:\n",
    "        normalised = np.zeros_like(res, dtype=np.uint8)\n",
    "    else:\n",
    "        normalised = 255 * (res - depth_min) / (depth_max - depth_min)\n",
    "        normalised = normalised.astype(np.uint8)\n",
    "    return normalised\n",
    "\n",
    "leaf_folders = list_leaf_folders(root_dir)\n",
    "for folder in tqdm(leaf_folders):\n",
    "    relative_path = os.path.relpath(folder, root_dir)\n",
    "    images = list_sorted_images(folder)\n",
    "    for i in range(len(images)):\n",
    "        img_path = images[i]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        res = calculate_depth_picture(img)\n",
    "        save_path = os.path.join(save_folder, relative_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        pil_img = Image.fromarray(res, mode='L')  \n",
    "        pil_img.save(os.path.join(save_path, f\"depth_image_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7e84",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Code for downloading the data is based on FaceForensics++ dataset and is available on https://github.com/ondyari/FaceForensics\n",
    "\n",
    "In the code below we simply sample the required batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08438f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device='cuda:0')\n",
    "\n",
    "def process_video(path, save_path):\n",
    "    print(path, save_path)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    faces = [frame_faces for i in tqdm(range(0, len(frames), BATCH_SIZE_DATA)) for frame_faces in mtcnn(frames[i:min(len(frames),i+BATCH_SIZE_DATA)])]\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for i, frame_faces in tqdm(enumerate(faces)):\n",
    "        if frame_faces is None:\n",
    "            continue\n",
    "        face = frame_faces[0]\n",
    "        img_pil = Image.fromarray(face.permute(1, 2, 0).numpy().astype(np.uint8))\n",
    "        img_pil.save(os.path.join(save_path, f\"frame_{i}.png\"))\n",
    "\n",
    "\n",
    "for dirpath, _, filenames in os.walk(video_root):\n",
    "    for file in filenames:\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(dirpath, file)\n",
    "            relative_path = os.path.relpath(video_path, video_root)\n",
    "            output_folder = os.path.join(data_root, os.path.splitext(relative_path)[0])\n",
    "\n",
    "            process_video(video_path, output_folder)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e025ca9",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_name = \"facebook/dinov2-base\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "backbone = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_video_folders(base_dir):\n",
    "    folders = []\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            # check if it contains frames\n",
    "            if any(fname.lower().endswith(('.jpg', '.png')) for fname in os.listdir(folder_path)):\n",
    "                folders.append(folder_path)\n",
    "    return sorted(folders)\n",
    "\n",
    "def extract_frame_paths_and_labels(base_dirs, label, train_ratio=0.8):\n",
    "    train_paths = []\n",
    "    test_paths = []\n",
    "\n",
    "    for base_dir in base_dirs:\n",
    "        video_folders = collect_video_folders(base_dir)\n",
    "        total_videos = len(video_folders)\n",
    "\n",
    "        train_cutoff = int(total_videos * train_ratio)\n",
    "\n",
    "        for i, video_folder in enumerate(video_folders):\n",
    "            frame_paths = glob.glob(os.path.join(video_folder, '*'))\n",
    "            frame_paths = [f for f in frame_paths if f.lower().endswith(('.jpg', '.png'))]\n",
    "\n",
    "            labeled = [(fp, label) for fp in frame_paths]\n",
    "\n",
    "            if i < train_cutoff:\n",
    "                train_paths.extend(labeled)\n",
    "            else:\n",
    "                test_paths.extend(labeled)\n",
    "\n",
    "    return train_paths, test_paths\n",
    "\n",
    "# Define your paths\n",
    "original_sources = [\n",
    "    'flow_data/original_sequences/actors/c23/videos',\n",
    "    'flow_data/original_sequences/youtube/c23/videos'\n",
    "]\n",
    "\n",
    "manipulated_sources = [\n",
    "    'flow_data/manipulated_sequences/DeepFakeDetection/c23/videos',\n",
    "    'flow_data/manipulated_sequences/Deepfakes/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/Face2Face/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/FaceShifter/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/FaceSwap/c23/videos',\n",
    "    # 'flow_data/manipulated_sequences/NeuralTextures/c23/videos'\n",
    "]\n",
    "\n",
    "# Collect data\n",
    "train_real, test_real = extract_frame_paths_and_labels(original_sources, label=0)\n",
    "train_fake, test_fake = extract_frame_paths_and_labels(manipulated_sources, label=1)\n",
    "\n",
    "# Combine\n",
    "train_data = train_real + train_fake\n",
    "test_data = test_real + test_fake\n",
    "\n",
    "# Optionally shuffle\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "# Sample output\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Testing samples: {len(test_data)}\")\n",
    "print(f\"Sample training item: {train_data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label, dtype=torch.float32) \n",
    "        return image, label\n",
    "    \n",
    "def repeat_channels(x):\n",
    "    if isinstance(x, torch.Tensor) and x.dim() == 3 and x.shape[0] == 1:\n",
    "        return x.repeat(3, 1, 1)\n",
    "    elif isinstance(x, torch.Tensor) and x.dim() == 2:\n",
    "        # Add channel dimension if missing\n",
    "        return x.unsqueeze(0).repeat(3, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected input shape for repeat_channels: {x.shape}\")\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = FrameDataset(train_data, transform=image_transform)\n",
    "test_dataset = FrameDataset(test_data, transform=image_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bffbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in backbone.named_parameters():\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoBinaryClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.config.hidden_size, 256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1, bias=True)  # binary output\n",
    "        )\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]  \n",
    "        return self.classifier(cls_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DinoBinaryClassifier(backbone)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4784356",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if \"backbone\" in name:\n",
    "            backbone_params.append(param)\n",
    "        else:\n",
    "            classifier_params.append(param)\n",
    "print(len(backbone_params), len(classifier_params))\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone_params, \"lr\": 1e-5},\n",
    "    {\"params\": classifier_params, \"lr\": 1e-4},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2acc9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in tqdm.tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        # print(outputs.mean().item(), outputs.std().item())\n",
    "        \n",
    "        correct += (preds == labels.bool()).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            correct += (preds == labels.bool()).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5403b6",
   "metadata": {},
   "source": [
    "#### Running the model with Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9810ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a47960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a single batch\n",
    "images, labels = next(iter(train_loader))\n",
    "for i in range(100):  # overfit\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images.to(device))\n",
    "    loss = criterion(outputs, labels.to(device).unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {i}, Loss: {loss.item()}, Output Std: {outputs.std().item()}, outputs mean {outputs.mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fad36e",
   "metadata": {},
   "source": [
    "#### Running the model with Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 1 + num_epochs):\n",
    "    save_path = f\"depth_epoch_{epoch}.pth\"\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a single batch\n",
    "images, labels = next(iter(train_loader))\n",
    "for i in range(100):  # overfit\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images.to(device))\n",
    "    loss = criterion(outputs, labels.to(device).unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {i}, Loss: {loss.item()}, Output Std: {outputs.std().item()}, outputs mean {outputs.mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc017fc1",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e48c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
