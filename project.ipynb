{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eec08ffb",
      "metadata": {
        "id": "eec08ffb"
      },
      "source": [
        "# Final Project Computer Vision\n",
        "## Truth in Motion: Depth and Flow Enhanced DeepFake Detection\n",
        "\n",
        "Authors: Aimee Lin, Neli Catar and Gellert Toth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b316f18a",
      "metadata": {
        "id": "b316f18a"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a0a224",
      "metadata": {
        "id": "66a0a224"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/DepthAnything/Depth-Anything-V2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20779f52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20779f52",
        "outputId": "9e3df681-3ef7-490a-9731-40b3bbe2057f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1138  100  1138    0     0   4115      0 --:--:-- --:--:-- --:--:--  4123\n",
            "100  371M  100  371M    0     0   121M      0  0:00:03  0:00:03 --:--:--  148M\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p DepthAnythingV2Checkpoints\n",
        "!curl -L https://huggingface.co/depth-anything/Depth-Anything-V2-Base/resolve/main/depth_anything_v2_vitb.pth?download=true --output DepthAnythingV2Checkpoints/depth_anything_v2_vitb.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c0e141",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c0e141",
        "outputId": "16eccf56-d308-42e2-affd-0e83b521ebb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-pwc'...\n",
            "remote: Enumerating objects: 251, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 251 (delta 40), reused 62 (delta 24), pack-reused 169 (from 1)\u001b[K\n",
            "Receiving objects: 100% (251/251), 66.96 MiB | 22.39 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sniklaus/pytorch-pwc.git\n",
        "!mv pytorch-pwc pytorch_pwc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nxaxGrshrOQK",
      "metadata": {
        "id": "nxaxGrshrOQK"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0c5f3dfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "0c5f3dfc",
        "outputId": "c5a5dadf-6e38-40bd-af2a-2f63fbfd75ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gellert/Documents/UNI/Computer_Vision_Final_Project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "xFormers not available\n",
            "xFormers not available\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from facenet_pytorch import MTCNN\n",
        "from IPython.display import display\n",
        "from matplotlib import colors\n",
        "from PIL import Image, ImageShow\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import time\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "depth_anything_path = os.path.abspath(\"Depth-Anything-V2\")\n",
        "\n",
        "if depth_anything_path not in sys.path:\n",
        "    sys.path.append(depth_anything_path)\n",
        "from depth_anything_v2.dpt import DepthAnythingV2\n",
        "\n",
        "argv_backup = sys.argv\n",
        "sys.argv = [sys.argv[0]]\n",
        "from pytorch_pwc.run import estimate\n",
        "sys.argv = argv_backup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8580cd0",
      "metadata": {},
      "source": [
        "Download data if running in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99l0Au36iu5K",
      "metadata": {
        "id": "99l0Au36iu5K"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "zip_path = '/content/drive/MyDrive/Colab Data/'\n",
        "extract_path = '/content/.'\n",
        "\n",
        "# Create the destination folder\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract\n",
        "with zipfile.ZipFile(os.path.join(zip_path, \"flow_data.zip\"), 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "with zipfile.ZipFile(os.path.join(zip_path, \"depth_data.zip\"), 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db574a14",
      "metadata": {
        "id": "db574a14"
      },
      "source": [
        "### Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ea4349aa",
      "metadata": {
        "id": "ea4349aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# data folders\n",
        "video_root = './data' # where the data is stored\n",
        "data_root = './preprocessed_data' # where the pre processed data gets stored\n",
        "data_flow = \"./flow_data\" # where flow data gets saved\n",
        "data_depth = \"./depth_data\"\n",
        "\n",
        "# data loading\n",
        "BATCH_SIZE = 64 # batch size of the preprocessed data\n",
        "\n",
        "# device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(DEVICE)\n",
        "# training\n",
        "num_epochs = 5 # epochs when training\n",
        "SKIP_FRAMES = 2 # When processing the video jump some frames to reduce data size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5106e7",
      "metadata": {
        "id": "2d5106e7"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c8b8a222",
      "metadata": {
        "id": "c8b8a222"
      },
      "outputs": [],
      "source": [
        "def split_video_into_frames(path, save_path):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    faces = [frame_faces for i in range(0, len(frames), BATCH_SIZE) for frame_faces in mtcnn(frames[i:min(len(frames),i+BATCH_SIZE)])]\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    for i, frame_faces in enumerate(faces):\n",
        "        if frame_faces is None:\n",
        "            continue\n",
        "        face = frame_faces[0]\n",
        "        img_pil = Image.fromarray(face.permute(1, 2, 0).numpy().astype(np.uint8))\n",
        "        img_pil.save(os.path.join(save_path, f\"frame_{i}.png\"))\n",
        "\n",
        "def list_leaf_folders(root_dir):\n",
        "    leaf_folders = []\n",
        "    for dirpath, dirnames, _ in os.walk(root_dir):\n",
        "        if not dirnames:\n",
        "            leaf_folders.append(dirpath)\n",
        "    return leaf_folders\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
        "\n",
        "def list_sorted_images(folder, extensions={'.png', '.jpg', '.jpeg', '.bmp'}):\n",
        "    files = [f for f in os.listdir(folder)\n",
        "             if os.path.isfile(os.path.join(folder, f)) and os.path.splitext(f)[1].lower() in extensions]\n",
        "    files.sort(key=natural_sort_key)\n",
        "    return [os.path.join(folder, f) for f in files]\n",
        "\n",
        "def flow_to_rgb(flow):\n",
        "    u = flow[..., 0]\n",
        "    v = flow[..., 1]\n",
        "\n",
        "    magnitude = np.sqrt(u**2 + v**2)\n",
        "    direction = np.arctan2(v, u)\n",
        "    mag_max = np.max(magnitude)\n",
        "    magnitude = magnitude / mag_max if mag_max != 0 else magnitude\n",
        "\n",
        "    direction = (direction + np.pi) / (2 * np.pi)\n",
        "    saturation = magnitude\n",
        "    value = np.ones_like(magnitude)\n",
        "    hsv_image = np.stack((direction, saturation, value), axis=-1)\n",
        "    rgb_image = colors.hsv_to_rgb(hsv_image.astype(np.float32))\n",
        "    return rgb_image\n",
        "\n",
        "def calculate_flow_picture(img1, img2):\n",
        "    res = estimate(img1, img2)\n",
        "    res = np.array(res.numpy(force=True).transpose(1, 2, 0), np.float32)\n",
        "    res = flow_to_rgb(res)\n",
        "    return res\n",
        "\n",
        "def calculate_depth_picture(model, img):\n",
        "    res = model.infer_image(img)\n",
        "    depth_min = np.min(res)\n",
        "    depth_max = np.max(res)\n",
        "    if depth_max - depth_min < 1e-6:\n",
        "        normalised = np.zeros_like(res, dtype=np.uint8)\n",
        "    else:\n",
        "        normalised = 255 * (res - depth_min) / (depth_max - depth_min)\n",
        "        normalised = normalised.astype(np.uint8)\n",
        "    return normalised\n",
        "\n",
        "def collect_video_folders(base_dir):\n",
        "    folders = []\n",
        "    for root, dirs, _ in os.walk(base_dir):\n",
        "        for d in dirs:\n",
        "            folder_path = os.path.join(root, d)\n",
        "            # check if it contains frames\n",
        "            if any(fname.lower().endswith(('.jpg', '.png')) for fname in os.listdir(folder_path)):\n",
        "                folders.append(folder_path)\n",
        "    return sorted(folders)\n",
        "\n",
        "def extract_frame_paths_and_labels(base_dirs, label, split_ratio = [0.7, 0.2, 0.1]):\n",
        "    train_paths = []\n",
        "    val_paths = []\n",
        "    test_paths = []\n",
        "\n",
        "    for base_dir in base_dirs:\n",
        "        video_folders = collect_video_folders(base_dir)\n",
        "        total_videos = len(video_folders)\n",
        "\n",
        "        train_cutoff = int(total_videos * split_ratio[0])\n",
        "        val_cutoff = int(total_videos * (split_ratio[0] + split_ratio[1]))\n",
        "\n",
        "        for i, video_folder in enumerate(video_folders):\n",
        "            frame_paths = glob.glob(os.path.join(video_folder, '*'))\n",
        "            frame_paths = [f for f in frame_paths if f.lower().endswith(('.jpg', '.png'))]\n",
        "\n",
        "            labeled = [(fp, label) for fp in frame_paths]\n",
        "\n",
        "            if i < train_cutoff:\n",
        "                train_paths.extend(labeled)\n",
        "            elif i < val_cutoff:\n",
        "                val_paths.extend(labeled)\n",
        "            else:\n",
        "                test_paths.extend(labeled)\n",
        "\n",
        "    return train_paths, val_paths, test_paths\n",
        "\n",
        "class FrameDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        return image, label\n",
        "\n",
        "def repeat_channels(x):\n",
        "    if isinstance(x, torch.Tensor) and x.dim() == 3 and x.shape[0] == 1:\n",
        "        return x.repeat(3, 1, 1)\n",
        "    elif isinstance(x, torch.Tensor) and x.dim() == 2:\n",
        "        # Add channel dimension if missing\n",
        "        return x.unsqueeze(0).repeat(3, 1, 1)\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected input shape for repeat_channels: {x.shape}\")\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.5, 0.5, 0.5]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device, scale=True):\n",
        "    model.train()\n",
        "    if scale:\n",
        "        scaler = GradScaler()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            if scale:\n",
        "                with autocast(dtype=torch.float16):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        \n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = probs > 0.5\n",
        "        correct += (preds == labels.bool()).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_labels.append(labels.cpu())\n",
        "        all_probs.append(probs.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_probs = torch.cat(all_probs).detach().numpy()\n",
        "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, scale=True):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).unsqueeze(1)\n",
        "            if scale:\n",
        "                with autocast(dtype=torch.float16):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = probs > 0.5\n",
        "            correct += (preds == labels.bool()).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_probs.append(probs.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_probs = torch.cat(all_probs).numpy()\n",
        "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
        "\n",
        "def plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs):\n",
        "    epochs = range(1, len(train_accuracies)+1)\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, train_accuracies, label='Train Acc')\n",
        "    plt.plot(epochs, val_accuracies, label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Accuracy')\n",
        "    plt.xticks(list(epochs))\n",
        "    plt.ylim(0.5, 1.0)\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, train_aurocs, label='Train AUROC')\n",
        "    plt.plot(epochs, val_aurocs, label='Val AUROC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUROC')\n",
        "    plt.legend()\n",
        "    plt.title('AUROC')\n",
        "    plt.xticks(list(epochs))\n",
        "    plt.ylim(0.5, 1.0)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_fp16_distilattion(model_learner, model_teacher, dataloader, criterion, optimizer, device):\n",
        "    scaler = GradScaler(device)\n",
        "    model_learner.train()\n",
        "    model_teacher.train()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for images, labels in tqdm.tqdm(dataloader):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            with autocast(dtype=torch.float16):\n",
        "                cls_token_teacher = model_teacher(images)\n",
        "                cls_token_teacher = cls_token_teacher.detach()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            with autocast(dtype=torch.float16):\n",
        "                cls_token_teacher_projected = model_learner.cls_projector(cls_token_teacher)\n",
        "                cls_token, outputs  = model_learner(images)\n",
        "\n",
        "                loss = criterion(outputs, labels, cls_token, cls_token_teacher_projected)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = probs > 0.5\n",
        "        correct += (preds == labels.bool()).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_labels.append(labels.cpu())\n",
        "        all_probs.append(probs.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_probs = torch.cat(all_probs).numpy()\n",
        "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
        "\n",
        "def evaluate_fp16_distilattion(model_learner, model_teacher, dataloader, criterion, device):\n",
        "    model_teacher.eval()\n",
        "    model_learner.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm.tqdm(dataloader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
        "\n",
        "            with autocast(dtype=torch.float16):\n",
        "                cls_token_teacher = model_teacher(images)\n",
        "                cls_token_teacher = cls_token_teacher.detach()\n",
        "                cls_token_teacher_projected = model_learner.cls_projector(cls_token_teacher)\n",
        "                cls_token, outputs = model_learner(images)\n",
        "\n",
        "                loss = criterion(outputs, labels, cls_token, cls_token_teacher_projected)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = probs > 0.5\n",
        "            correct += (preds == labels.bool()).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_probs.append(probs.cpu())\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_probs = torch.cat(all_probs).numpy()\n",
        "    return epoch_loss, epoch_acc, all_labels, all_probs\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.distill_loss = self._kl_loss\n",
        "\n",
        "    def forward(self, student_output, labels, student_cls, teacher_cls):\n",
        "        cls_loss = self.bce(student_output, labels)\n",
        "        distill = self.distill_loss(student_cls, teacher_cls)\n",
        "        total_loss = self.alpha * cls_loss + (1 - self.alpha) * distill\n",
        "        return total_loss\n",
        "\n",
        "    def _kl_loss(self, student_cls, teacher_cls):\n",
        "        T = self.temperature\n",
        "        student_log_probs = F.log_softmax(student_cls / T, dim=-1)\n",
        "        teacher_probs = F.softmax(teacher_cls / T, dim=-1)\n",
        "        return F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (T * T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b857a2",
      "metadata": {
        "id": "b3b857a2"
      },
      "source": [
        "#### Feature extraction generation\n",
        "\n",
        "In the sections below we tested the feature extraction generation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf7c0c7",
      "metadata": {
        "id": "cbf7c0c7"
      },
      "source": [
        "### Data\n",
        "\n",
        "Code for downloading the data is based on FaceForensics++ dataset and is available on https://github.com/ondyari/FaceForensics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd3304f",
      "metadata": {
        "id": "9dd3304f"
      },
      "source": [
        "1. Splitting the videos into frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1e8c2d",
      "metadata": {
        "id": "4f1e8c2d"
      },
      "outputs": [],
      "source": [
        "mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device=DEVICE)\n",
        "\n",
        "for dirpath, _, filenames in os.walk(video_root):\n",
        "    if len(filenames) == 0:\n",
        "        continue\n",
        "    print(f\"Processing videos in {dirpath}\")\n",
        "    for file in tqdm(filenames):\n",
        "        if file.endswith('.mp4'):\n",
        "            video_path = os.path.join(dirpath, file)\n",
        "            relative_path = os.path.relpath(video_path, video_root)\n",
        "            output_folder = os.path.join(data_root, os.path.splitext(relative_path)[0])\n",
        "            split_video_into_frames(video_path, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609f238c",
      "metadata": {
        "id": "609f238c"
      },
      "source": [
        "\n",
        "\n",
        "2. Calculating Optical Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3681431a",
      "metadata": {
        "id": "3681431a"
      },
      "outputs": [],
      "source": [
        "leaf_folders = list_leaf_folders(data_root)\n",
        "print(leaf_folders)\n",
        "for folder in tqdm(leaf_folders):\n",
        "    relative_path = os.path.relpath(folder, data_root)\n",
        "    images = list_sorted_images(folder)\n",
        "    images = [torch.FloatTensor(np.ascontiguousarray(np.array(Image.open(image))[:, :, ::-1].transpose(2, 0, 1).astype(np.float32) * (1.0 / 255.0))) for image in images]\n",
        "    for i in range(0, len(images)-1, SKIP_FRAMES):\n",
        "        img1, img2 = images[i], images[i+1]\n",
        "        res = calculate_flow_picture(img1, img2)\n",
        "        save_path = os.path.join(data_flow, relative_path)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        rgb_flow_uint8 = (res * 255).astype(np.uint8)\n",
        "        pil_img = Image.fromarray(rgb_flow_uint8, mode='RGB')\n",
        "        pil_img.save(os.path.join(save_path, f\"flow_image_{i}.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d204d08",
      "metadata": {
        "id": "8d204d08"
      },
      "source": [
        "3. Depth Estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f367d4c7",
      "metadata": {
        "id": "f367d4c7",
        "outputId": "e288a852-b6b8-41fc-d46c-d86432b2a259"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [2:57:51<00:00, 26.68s/it]  \n"
          ]
        }
      ],
      "source": [
        "model_configs = {\n",
        "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
        "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
        "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
        "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
        "}\n",
        "\n",
        "model = DepthAnythingV2(**model_configs['vitb'])\n",
        "model.load_state_dict(torch.load(f'DepthAnythingV2Checkpoints/depth_anything_v2_vitb.pth', map_location='cpu'))\n",
        "model = model.to(DEVICE).eval()\n",
        "\n",
        "\n",
        "leaf_folders = list_leaf_folders(data_root)\n",
        "for folder in tqdm(leaf_folders):\n",
        "    relative_path = os.path.relpath(folder, data_root)\n",
        "    images = list_sorted_images(folder)\n",
        "    for i in range(0, len(images), SKIP_FRAMES):\n",
        "        img_path = images[i]\n",
        "        img = np.array(Image.open(img_path))\n",
        "        res = calculate_depth_picture(model, img)\n",
        "        save_path = os.path.join(data_depth, relative_path)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        pil_img = Image.fromarray(res, mode='L')\n",
        "        pil_img.save(os.path.join(save_path, f\"depth_image_{i}.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e025ca9",
      "metadata": {
        "id": "5e025ca9"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0a0c5915",
      "metadata": {
        "id": "0a0c5915"
      },
      "outputs": [],
      "source": [
        "class DinoBinaryClassifier(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.backbone.config.hidden_size, 256, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1, bias=True)  # binary output\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for layer in self.classifier:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "                init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.backbone(pixel_values=pixel_values)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        return self.classifier(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "99958610",
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data_loaders(original_sources, manipulated_sources):\n",
        "    # Collect data\n",
        "    train_real, val_real, test_real = extract_frame_paths_and_labels(original_sources, label=0)\n",
        "    train_fake, val_fake, test_fake = extract_frame_paths_and_labels(manipulated_sources, label=1)\n",
        "\n",
        "    # Combine\n",
        "    train_data = train_real + train_fake\n",
        "    val_data = val_real + val_fake\n",
        "    test_data = test_real + test_fake\n",
        "\n",
        "    # Sample output\n",
        "    print(f\"Training samples: {len(train_data)}\")\n",
        "    print(f\"Testing samples: {len(test_data)}\")\n",
        "    print(f\"Sample training item: {train_data[0]}\")\n",
        "\n",
        "    train_dataset = FrameDataset(train_data, transform=image_transform)\n",
        "    val_dataset = FrameDataset(val_data, transform=image_transform)\n",
        "    test_dataset = FrameDataset(test_data, transform=image_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def plot_auc_curve(y_true, y_scores, title):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"AUC = {roc_auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def train_full_model(model, original_sources, manipulated_sources, model_name, scale=True):\n",
        "\n",
        "    train_loader, val_loader, test_loader = prepare_data_loaders(original_sources, manipulated_sources)\n",
        "\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    train_aurocs, val_aurocs = [], []\n",
        "    epoch_times = []\n",
        "\n",
        "    start_epoch = 1\n",
        "    num_epochs = 5\n",
        "\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    backbone_params = []\n",
        "    classifier_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if \"backbone\" in name:\n",
        "                backbone_params.append(param)\n",
        "            else:\n",
        "                classifier_params.append(param)\n",
        "    print(len(backbone_params), len(classifier_params))\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {\"params\": backbone_params, \"lr\": 1e-5},\n",
        "        {\"params\": classifier_params, \"lr\": 1e-4},\n",
        "    ])\n",
        "    best_epoch, best_val = -1, 1e9 \n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        save_path = os.path.join(\"checkpoints\", f\"{model_name}_epoch_{epoch}.pth\")\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc, train_labels, train_probs = train(model, train_loader, criterion, optimizer, DEVICE, scale)\n",
        "        val_loss, val_acc, val_labels, val_probs = evaluate(model, val_loader, criterion, DEVICE, scale)\n",
        "        \n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss,\n",
        "            best_epoch = epoch\n",
        "        # AUROC\n",
        "        train_auroc = roc_auc_score(train_labels, train_probs)\n",
        "        val_auroc = roc_auc_score(val_labels, val_probs)\n",
        "\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "        train_aurocs.append(train_auroc)\n",
        "        val_aurocs.append(val_auroc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
        "        print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "        }, save_path)\n",
        "\n",
        "    plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs)\n",
        "    model_data = torch.load( os.path.join(\"checkpoints\", f\"{model_name}_{best_epoch}.pth\"))\n",
        "    model.load_state_dict(model_data[\"model_state_dict\"])\n",
        "    start = time.time()\n",
        "    test_loss, test_acc, test_labels, test_probs = evaluate(model, test_loader, criterion, DEVICE, scale)\n",
        "    runtime = time.time() - start\n",
        "    plot_auc_curve(test_labels, test_probs, title=f\"AUC Curve, eval done in {runtime:.4f} seconds, accuracy: {test_acc:.4f}\")\n",
        "    \n",
        "\n",
        "def train_full_model_distillation(model, teacher_model, original_sources, manipulated_sources, model_name):\n",
        "    train_loader, val_loader, test_loader = prepare_data_loaders(original_sources, manipulated_sources)\n",
        "\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    train_aurocs, val_aurocs = [], []\n",
        "    epoch_times = []\n",
        "\n",
        "    start_epoch = 1\n",
        "    num_epochs = 5\n",
        "\n",
        "    criterion = DistillationLoss()\n",
        "    backbone_params = []\n",
        "    classifier_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if \"backbone\" in name:\n",
        "                backbone_params.append(param)\n",
        "            else:\n",
        "                classifier_params.append(param)\n",
        "    print(len(backbone_params), len(classifier_params))\n",
        "    optimizer = torch.optim.Adam([\n",
        "        {\"params\": backbone_params, \"lr\": 1e-5},\n",
        "        {\"params\": classifier_params, \"lr\": 1e-4},\n",
        "    ])\n",
        "    best_epoch, best_val = -1, 1e9 \n",
        "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
        "        save_path = os.path.join(\"checkpoints\", f\"{model_name}_epoch_{epoch}.pth\")\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc, train_labels, train_probs = train_fp16_distilattion(model, teacher_model, train_loader, criterion, optimizer, DEVICE)\n",
        "        val_loss, val_acc, val_labels, val_probs = evaluate_fp16_distilattion(model, teacher_model, val_loader, criterion, DEVICE)\n",
        "        \n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss,\n",
        "            best_epoch = epoch\n",
        "        # AUROC\n",
        "        train_auroc = roc_auc_score(train_labels, train_probs)\n",
        "        val_auroc = roc_auc_score(val_labels, val_probs)\n",
        "\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "        train_aurocs.append(train_auroc)\n",
        "        val_aurocs.append(val_auroc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train AUROC: {train_auroc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val AUROC: {val_auroc:.4f}\")\n",
        "        print(f\"Epoch time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "        }, save_path)\n",
        "\n",
        "    plot_training_progress(train_accuracies, val_accuracies, train_aurocs, val_aurocs)\n",
        "    model_data = torch.load( os.path.join(\"checkpoints\", f\"{model_name}_{best_epoch}.pth\"))\n",
        "    model.load_state_dict(model_data[\"model_state_dict\"])\n",
        "    start = time.time()\n",
        "    test_loss, test_acc, test_labels, test_probs = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    runtime = time.time() - start\n",
        "    plot_auc_curve(test_labels, test_probs, title=f\"AUC Curve, eval done in {runtime:.4f} seconds, accuracy: {test_acc:.4f}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd398ae",
      "metadata": {
        "id": "5cd398ae"
      },
      "source": [
        "#### Flow based network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5d16bf",
      "metadata": {
        "id": "5f5d16bf"
      },
      "source": [
        "Preparing the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d171386d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d171386d",
        "outputId": "d59ab3d3-9344-4692-cfd6-39f008f14205"
      },
      "outputs": [],
      "source": [
        "# Define your paths\n",
        "original_sources_flow = [\n",
        "    'flow_data/original_sequences/actors/c23/videos',\n",
        "    'flow_data/original_sequences/youtube/c23/videos'\n",
        "]\n",
        "\n",
        "manipulated_sources_flow = [\n",
        "    'flow_data/manipulated_sequences/DeepFakeDetection/c23/videos',\n",
        "    'flow_data/manipulated_sequences/Deepfakes/c23/videos',\n",
        "    # 'flow_data/manipulated_sequences/Face2Face/c23/videos',\n",
        "    # 'flow_data/manipulated_sequences/FaceShifter/c23/videos',\n",
        "    # 'flow_data/manipulated_sequences/FaceSwap/c23/videos',\n",
        "    # 'flow_data/manipulated_sequences/NeuralTextures/c23/videos'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cc74e757",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc74e757",
        "outputId": "c3627209-7686-48c0-b02e-9f16b1559cf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"facebook/dinov2-base\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in backbone.named_parameters():\n",
        "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = DinoBinaryClassifier(backbone)\n",
        "model.to(DEVICE);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ccbd3c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "9ccbd3c2",
        "outputId": "5eb1e7e0-e8c2-4586-c8c0-4365796dbcbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 180801\n",
            "Testing samples: 32243\n",
            "Sample training item: ('flow_data/original_sequences/actors/c23/videos/01__hugging_happy/flow_image_485.png', 0)\n",
            "36 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 116/1413 [01:00<11:12,  1.93it/s]"
          ]
        }
      ],
      "source": [
        "train_full_model(model, original_sources_flow, manipulated_sources_flow, \"flow_dinov2_base_fp16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43949af",
      "metadata": {
        "id": "c43949af"
      },
      "source": [
        "#### Depth Based network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c41592",
      "metadata": {
        "id": "74c41592",
        "outputId": "57f6238d-2e89-4807-9267-4eed42056c65"
      },
      "outputs": [],
      "source": [
        "# Define your paths\n",
        "original_sources_depth = [\n",
        "    'depth_data/original_sequences/actors/c23/videos',\n",
        "    'depth_data/original_sequences/youtube/c23/videos'\n",
        "]\n",
        "\n",
        "manipulated_sources_depth = [\n",
        "    'depth_data/manipulated_sequences/DeepFakeDetection/c23/videos',\n",
        "    'depth_data/manipulated_sequences/Deepfakes/c23/videos',\n",
        "    # 'depth_data/manipulated_sequences/Face2Face/c23/videos',\n",
        "    # 'depth_data/manipulated_sequences/FaceShifter/c23/videos',\n",
        "    # 'depth_data/manipulated_sequences/FaceSwap/c23/videos',\n",
        "    # 'depth_data/manipulated_sequences/NeuralTextures/c23/videos'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9aadae9",
      "metadata": {
        "id": "b9aadae9"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/dinov2-base\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in backbone.named_parameters():\n",
        "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = DinoBinaryClassifier(backbone)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69827b83",
      "metadata": {
        "id": "69827b83"
      },
      "outputs": [],
      "source": [
        "train_full_model(model, original_sources_depth, manipulated_sources_depth, \"depth_dinov2_base_fp16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b8e1aa6",
      "metadata": {
        "id": "2b8e1aa6"
      },
      "source": [
        "### Compression methods\n",
        "\n",
        "1. Quantization\n",
        "\n",
        "The models above were trained using fp16 instead of fp32 using autocast. This automatically uses fp16 when it is safe and uses fp32 when it is required. This promises almost no loss in performance while making the model more efficient. To test this we can train a model while turning this feature off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59af9717",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"facebook/dinov2-base\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in backbone.named_parameters():\n",
        "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = DinoBinaryClassifier(backbone)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84ab1b1",
      "metadata": {
        "id": "d84ab1b1"
      },
      "outputs": [],
      "source": [
        "train_full_model(model, original_sources_flow, manipulated_sources_flow, \"flow_dinov2_base_fp32\", scale=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf08f6aa",
      "metadata": {
        "id": "cf08f6aa"
      },
      "source": [
        "2. Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e63bf06c",
      "metadata": {},
      "source": [
        "In order to test whether distilling a larger model helps we have to train the smaller model by itself first so we have a point of comparison. Then train it again but this time include in the loss the difference between the representations of the learner and teacher model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ba6289",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"facebook/dinov2-small\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "backbone = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "for param in backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in backbone.named_parameters():\n",
        "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = DinoBinaryClassifier(backbone)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2211b0a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_full_model(model, original_sources_flow, manipulated_sources_flow, \"flow_dinov2_small_fp16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee1aedb",
      "metadata": {
        "id": "8ee1aedb"
      },
      "outputs": [],
      "source": [
        "class DinoBinaryClassifierForDistilattion(DinoBinaryClassifier):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__(backbone)\n",
        "        self.cls_projector = nn.Linear(1024, 384)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.backbone(pixel_values=pixel_values)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        return cls_token, self.classifier(cls_token)\n",
        "\n",
        "\n",
        "class DinoBinaryClassifierForDistilattionTeacher(DinoBinaryClassifier):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__(backbone)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        outputs = self.backbone(pixel_values=pixel_values)\n",
        "        cls_token = outputs.last_hidden_state[:, 0]\n",
        "        return cls_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388a7ea8",
      "metadata": {
        "id": "388a7ea8"
      },
      "outputs": [],
      "source": [
        "large_model_name = \"facebook/dinov2-large\"\n",
        "small_model_name = \"facebook/dinov2-small\"\n",
        "\n",
        "backbone_large = AutoModel.from_pretrained(large_model_name)\n",
        "backbone_small = AutoModel.from_pretrained(small_model_name)\n",
        "\n",
        "for param in backbone_small.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for name, param in backbone_small.named_parameters():\n",
        "    if \"encoder.layer.11\" in name or \"encoder.layer.10\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model_teacher = DinoBinaryClassifierForDistilattionTeacher(backbone_large)\n",
        "model_teacher.load_state_dict(torch.load('/checkpoints/flow_dinov2_base_fp16_epoch_10.pth')[\"model_state_dict\"])\n",
        "model_teacher.to(DEVICE)\n",
        "\n",
        "model_learner = DinoBinaryClassifierForDistilattion(backbone_small)\n",
        "model_learner.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a734f56",
      "metadata": {
        "id": "4a734f56"
      },
      "outputs": [],
      "source": [
        "train_full_model_distillation(model_learner, model_teacher, original_sources_flow, manipulated_sources_flow, model_name=\"flow_dinov2_small_fp16_distilled\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
